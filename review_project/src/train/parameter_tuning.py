import optuna
import torch
import os
import json
import pandas as pd
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments
from peft import LoraConfig, get_peft_model, PeftModel
from trl import SFTTrainer
from datasets import Dataset
from transformers import EarlyStoppingCallback
import gc

# CUDA ë””ë°”ì´ìŠ¤ ì„¤ì •
os.environ["CUDA_VISIBLE_DEVICES"] = "0"  # GPU IDë¥¼ ì„¤ì •
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
os.environ["FLASH_ATTENTION"] = "1"

# ë°ì´í„° ë¡œë“œ
train = pd.read_csv('/home/yooyoung/dacon/train_replace_insert_char.csv', encoding='utf-8-sig')
train = train.drop(columns=['Unnamed: 0', 'ID'])  # id ì»¬ëŸ¼ ì œê±°
dataset = Dataset.from_pandas(train)

train_test_split = dataset.train_test_split(test_size=0.2, seed=42)

train_dataset = train_test_split['train']
test_dataset = train_test_split['test']

# ëª¨ë¸ ë¡œë“œ
model_name = "MLP-KTLim/llama-3-Korean-Bllossom-8B"
device = "cuda" if torch.cuda.is_available() else "cpu"

# Tokenizer ì„¤ì •
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = 'right'

# Prompt ìƒì„± í•¨ìˆ˜
def create_prompt(input, output):
    prompt = (
        "<start_of_turn> Your task is to transform the given obfuscated Korean review into a clear, correct, and natural-sounding Korean review that reflects its original meaning.\n"
        f"Input: {input}\n"
        "<end_of_turn>\n"
        "<start_of_turn>Assistant:\n"
        f"Output: {output}"
    )
    return prompt

# ë°ì´í„°ì…‹ í¬ë§·íŒ…
def format_chat_template(row):
    prompt = create_prompt(row["input"], row["output"])
    tokens = tokenizer.encode(prompt, truncation=True, max_length=768)
    row["input_ids"] = tokens
    return row

train_dataset = train_dataset.map(format_chat_template, batched=False, num_proc=1)
test_dataset = test_dataset.map(format_chat_template, batched=False, num_proc=1)

# ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì €ì¥ íŒŒì¼ ê²½ë¡œ
BEST_PARAMS_FILE = "/home/yooyoung/dacon/best_hyperparams.json"

def objective(trial):
    global BEST_PARAMS_FILE  # ìµœì  íŒŒë¼ë¯¸í„° ì €ì¥ ê²½ë¡œ

    # í•™ìŠµ ì‹œì‘ ì „ ë©”ëª¨ë¦¬ ì •ë¦¬
    torch.cuda.empty_cache()
    gc.collect()

    # íŠœë‹í•  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì •ì˜
    learning_rate = trial.suggest_loguniform("learning_rate", 1e-5, 5e-4)
    lora_r = trial.suggest_categorical("lora_r", [8, 16]) 
    lora_alpha = trial.suggest_categorical("lora_alpha", [16, 32, 64])
    lora_dropout = trial.suggest_uniform("lora_dropout", 0.05, 0.3)

    # ëª¨ë¸ ë¡œë“œ (float16 ì ìš©)
    model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).to(device)

    # LoRA ì„¤ì •
    lora_config = LoraConfig(
        r=lora_r,
        lora_alpha=lora_alpha,
        target_modules=["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "down_proj", "up_proj"],
        lora_dropout=lora_dropout,
        bias='none',
        task_type='CAUSAL_LM'
    )

    # LoRA ì ìš©
    model = get_peft_model(model, lora_config)
    model.train()

    # TrainingArguments ì„¤ì •
    training_args = TrainingArguments(
        output_dir="./results_tuning",
        num_train_epochs=3,  
        per_device_train_batch_size=1,
        per_device_eval_batch_size=1,
        gradient_accumulation_steps=4,  # ê¸°ì¡´ 2 â†’ 4 (ë©”ëª¨ë¦¬ ì ˆì•½)
        optim="adamw_bnb_8bit",
        learning_rate=learning_rate,
        eval_strategy="steps",
        eval_steps=500,
        save_strategy="steps",
        save_steps=1000,
        save_total_limit=1,
        logging_dir="./logs",
        fp16=True,
        load_best_model_at_end=True,
    )

    # Early Stopping ì¶”ê°€
    early_stopping = EarlyStoppingCallback(early_stopping_patience=2)

    # Trainer ì„¤ì •
    trainer = SFTTrainer(
        model=model,
        train_dataset=train_dataset,
        eval_dataset=test_dataset,
        args=training_args,
        peft_config=lora_config,
        callbacks=[early_stopping]
    )

    # í•™ìŠµ ì‹œì‘
    trainer.train()

    # í‰ê°€ (eval_loss ë°˜í™˜)
    eval_loss = trainer.evaluate()["eval_loss"]

    # ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì €ì¥ (ì†ì‹¤ ê°’ í¬í•¨)
    best_params = trial.params
    best_params["eval_loss"] = eval_loss  # ì†ì‹¤ ê°’ ì¶”ê°€

    with open(BEST_PARAMS_FILE, "w") as f:
        json.dump(best_params, f, indent=4)

    print(f"ğŸ”¹ í˜„ì¬ ìµœì  íŒŒë¼ë¯¸í„°: {best_params}")

    # ë©”ëª¨ë¦¬ ì •ë¦¬ (ëª¨ë¸ ì‚­ì œ)
    del model
    torch.cuda.empty_cache()
    gc.collect()

    return eval_loss  # Optunaê°€ ìµœì†Œí™”í•  ê°’ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)

# Optuna ìµœì í™” ì‹¤í–‰ (Pruner ì ìš©)
study = optuna.create_study(direction="minimize")
study.optimize(objective, n_trials=5)  # 5ë²ˆ ì‹œë„

# ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¶œë ¥
print("Best Hyperparameters:", study.best_params)

# ì¤‘ê°„ ì €ì¥ëœ ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¡œë“œ
with open(BEST_PARAMS_FILE, "r") as f:
    best_params = json.load(f)

# # ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì €ì¥
# best_params = study.best_params

# ìµœì  ì¡°í•©ìœ¼ë¡œ ëª¨ë¸ ì¬í•™ìŠµ
model = AutoModelForCausalLM.from_pretrained(model_name).to(device)

lora_config = LoraConfig(
    r=best_params["lora_r"],
    lora_alpha=best_params["lora_alpha"],
    target_modules=["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "down_proj", "up_proj"],
    lora_dropout=best_params["lora_dropout"],
    bias='none',
    task_type='CAUSAL_LM'
)

model = get_peft_model(model, lora_config)
model.train()

training_args = TrainingArguments(
    output_dir="./results_best",
    num_train_epochs=10,  # ìµœì¢… í•™ìŠµì—ì„œëŠ” 10 Epoch
    per_device_train_batch_size=1,
    per_device_eval_batch_size=1,
    gradient_accumulation_steps=4,
    learning_rate=best_params["learning_rate"],
    eval_strategy="steps",
    eval_steps=500,
    save_strategy="steps",
    save_steps=1000,
    save_total_limit=2,  # ì €ì¥í•  ì²´í¬í¬ì¸íŠ¸ ìˆ˜ ì œí•œ
    logging_dir="./logs",
    fp16=True,
    load_best_model_at_end=True,
)

trainer = SFTTrainer(
    model=model,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    args=training_args,
    peft_config=lora_config
)

# ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ê¸°ë°˜ ìµœì¢… í•™ìŠµ ì‹œì‘
trainer.train()

# ìµœì¢… ëª¨ë¸ ì €ì¥
ADAPTER_MODEL = "/home/yooyoung/dacon/llama-3.2-Korean-Bllossom-8B_char_best_hyperparams"
trainer.model.save_pretrained(ADAPTER_MODEL)

BASE_MODEL = "MLP-KTLim/llama-3-Korean-Bllossom-8B"
model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, device_map='auto', torch_dtype=torch.float16)
model = PeftModel.from_pretrained(model, ADAPTER_MODEL, device_map='auto', torch_dtype=torch.float16)

# ìµœì  ëª¨ë¸ ì €ì¥
FINAL_MODEL_PATH = "/home/yooyoung/dacon/llama-3.2-Korean-Bllossom-8B_char_final"
model.save_pretrained(FINAL_MODEL_PATH)
tokenizer.save_pretrained(FINAL_MODEL_PATH)